# LLM SDK - ç»Ÿä¸€çš„å¤§æ¨¡å‹è°ƒç”¨æ¥å£

ä¸€ä¸ªç®€å•æ˜“ç”¨çš„Python SDKï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£æ¥è°ƒç”¨å¤šä¸ªå¤§æ¨¡å‹å¹³å°çš„APIã€‚

## ç‰¹æ€§

- ğŸš€ **ç»Ÿä¸€æ¥å£**: ä¸€å¥—APIé€‚é…å¤šä¸ªå¹³å°
- ğŸ”Œ **å¤šå¹³å°æ”¯æŒ**: ARK(è±†åŒ…)ã€OpenAIã€Anthropic Claudeã€é€šä¹‰åƒé—®ã€DeepSeekç­‰
- ğŸ’¡ **ç®€å•æ˜“ç”¨**: ç®€æ´çš„APIè®¾è®¡ï¼Œå¿«é€Ÿä¸Šæ‰‹
- ğŸŒŠ **æµå¼è¾“å‡º**: æ”¯æŒæµå¼å“åº”
- ğŸ›  **é«˜åº¦å¯é…ç½®**: æ”¯æŒè‡ªå®šä¹‰URLã€å‚æ•°ç­‰

## æ”¯æŒçš„å¹³å°

| å¹³å° | æ ‡è¯†ç¬¦ | è¯´æ˜ |
|------|--------|------|
| ARK (è±†åŒ…) | `ark` | å­—èŠ‚è·³åŠ¨ç«å±±å¼•æ“ARKå¹³å° |
| OpenAI | `openai` | OpenAIå®˜æ–¹API |
| Anthropic | `anthropic` | Anthropic Claude |
| é€šä¹‰åƒé—® | `qwen` | é˜¿é‡Œäº‘é€šä¹‰åƒé—® |
| æ–‡å¿ƒä¸€è¨€ | `ernie` | ç™¾åº¦æ–‡å¿ƒä¸€è¨€ |
| æ™ºè°±AI | `zhipu` | æ™ºè°±AI GLMç³»åˆ— |
| DeepSeek | `deepseek` | DeepSeek |

## å®‰è£…

### ä¾èµ–

```bash
pip install requests
```

### ä½¿ç”¨

å°† `llm_sdk` ç›®å½•å¤åˆ¶åˆ°ä½ çš„é¡¹ç›®ä¸­å³å¯ã€‚

## å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
from llm_sdk import create_client, Message

# åˆ›å»ºå®¢æˆ·ç«¯
client = create_client(
    platform="ark",  # æˆ– "openai", "anthropic" ç­‰
    api_key="your-api-key"
)

# ç®€å•å¯¹è¯
response = client.simple_chat(
    prompt="ä½ å¥½ï¼è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚",
    model="your-model-id"
)
print(response)
```

### 2. å®Œæ•´æ¥å£

```python
from llm_sdk import create_client, Message

client = create_client(platform="openai", api_key="sk-...")

# æ„å»ºæ¶ˆæ¯
messages = [
    Message(role="system", content="ä½ æ˜¯ä¸€ä¸ªPythonä¸“å®¶ã€‚"),
    Message(role="user", content="å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•")
]

# è°ƒç”¨API
response = client.chat_completion(
    messages=messages,
    model="gpt-3.5-turbo",
    temperature=0.7,
    max_tokens=1000
)

print(f"å›å¤: {response.content}")
print(f"Tokenä½¿ç”¨: {response.usage}")
```

### 3. æµå¼è¾“å‡º

```python
client = create_client(platform="qwen", api_key="sk-...")

messages = [Message(role="user", content="ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½")]

# æµå¼è¾“å‡º
for chunk in client.stream_chat_completion(
    messages=messages,
    model="qwen-turbo"
):
    print(chunk, end="", flush=True)
```

## å¹³å°é…ç½®ç¤ºä¾‹

### ARK (è±†åŒ…)

```python
client = create_client(
    platform="ark",
    api_key="your-ark-api-key"
)

response = client.simple_chat(
    prompt="ä½ å¥½",
    model="your-endpoint-id"  # ARKçš„æ¨¡å‹ID
)
```

è·å–API Keyå’ŒEndpoint ID:
1. è®¿é—® [ç«å±±å¼•æ“æ§åˆ¶å°](https://console.volcengine.com/ark)
2. åˆ›å»ºæ¨ç†æ¥å…¥ç‚¹ï¼Œè·å–Endpoint ID
3. åœ¨APIç®¡ç†é¡µé¢è·å–API Key

### OpenAI

```python
client = create_client(
    platform="openai",
    api_key="sk-..."
)

response = client.simple_chat(
    prompt="Hello!",
    model="gpt-3.5-turbo"  # æˆ– gpt-4
)
```

### é€šä¹‰åƒé—®

```python
client = create_client(
    platform="qwen",
    api_key="sk-..."  # ä»é˜¿é‡Œäº‘è·å–
)

response = client.simple_chat(
    prompt="ä½ å¥½",
    model="qwen-turbo"  # æˆ– qwen-plus, qwen-max
)
```

### DeepSeek

```python
client = create_client(
    platform="deepseek",
    api_key="sk-..."
)

response = client.simple_chat(
    prompt="å†™ä¸€æ®µä»£ç ",
    model="deepseek-chat"
)
```

### Anthropic Claude

```python
client = create_client(
    platform="anthropic",
    api_key="sk-ant-..."
)

response = client.chat_completion(
    messages=[Message(role="user", content="Hello")],
    model="claude-3-sonnet-20240229",
    max_tokens=1024  # Claudeå¿…éœ€å‚æ•°
)
```

## è‡ªå®šä¹‰é…ç½®

### ä½¿ç”¨è‡ªå®šä¹‰URL

å¦‚æœä½¿ç”¨ä»£ç†æˆ–ä¸­è½¬æœåŠ¡:

```python
client = create_client(
    platform="openai",
    api_key="sk-...",
    base_url="https://your-proxy.com/v1"  # è‡ªå®šä¹‰URL
)
```

### ä¼ é€’é¢å¤–å‚æ•°

```python
response = client.chat_completion(
    messages=messages,
    model="gpt-3.5-turbo",
    temperature=0.8,
    top_p=0.9,
    presence_penalty=0.6,
    frequency_penalty=0.5
)
```

## APIå‚è€ƒ

### create_client()

åˆ›å»ºå¤§æ¨¡å‹å®¢æˆ·ç«¯ã€‚

```python
create_client(
    platform: str,        # å¹³å°æ ‡è¯†ç¬¦
    api_key: str,         # APIå¯†é’¥
    base_url: str = None, # è‡ªå®šä¹‰URLï¼ˆå¯é€‰ï¼‰
    **kwargs              # å…¶ä»–å‚æ•°
) -> BaseLLMClient
```

### simple_chat()

ç®€åŒ–çš„å¯¹è¯æ¥å£ã€‚

```python
client.simple_chat(
    prompt: str,                    # ç”¨æˆ·æç¤ºè¯
    system_prompt: str = None,      # ç³»ç»Ÿæç¤ºè¯
    model: str = None,              # æ¨¡å‹ID
    **kwargs                        # å…¶ä»–å‚æ•°
) -> str
```

### chat_completion()

å®Œæ•´çš„å¯¹è¯è¡¥å…¨æ¥å£ã€‚

```python
client.chat_completion(
    messages: List[Message],        # æ¶ˆæ¯åˆ—è¡¨
    model: str,                     # æ¨¡å‹ID
    temperature: float = 0.7,       # æ¸©åº¦
    max_tokens: int = None,         # æœ€å¤§tokenæ•°
    stream: bool = False,           # æ˜¯å¦æµå¼
    **kwargs                        # å…¶ä»–å‚æ•°
) -> ChatCompletionResponse
```

### stream_chat_completion()

æµå¼å¯¹è¯è¡¥å…¨ã€‚

```python
client.stream_chat_completion(
    messages: List[Message],
    model: str,
    temperature: float = 0.7,
    max_tokens: int = None,
    **kwargs
) -> Iterator[str]
```

## å®Œæ•´ç¤ºä¾‹

æŸ¥çœ‹ `llm_sdk_examples.py` æ–‡ä»¶è·å–æ›´å¤šç¤ºä¾‹ã€‚

è¿è¡Œç¤ºä¾‹:

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export ARK_API_KEY='your-ark-key'
export ARK_MODEL_ID='your-model-id'

# è¿è¡Œç¤ºä¾‹
python llm_sdk_examples.py
```

## å¸¸è§é—®é¢˜

### 1. å¦‚ä½•è·å–API Key?

- **ARK**: [ç«å±±å¼•æ“æ§åˆ¶å°](https://console.volcengine.com/ark)
- **OpenAI**: [OpenAI API Keys](https://platform.openai.com/api-keys)
- **é€šä¹‰åƒé—®**: [é˜¿é‡Œäº‘æ§åˆ¶å°](https://dashscope.console.aliyun.com/)
- **DeepSeek**: [DeepSeekå¹³å°](https://platform.deepseek.com/)
- **Anthropic**: [Anthropic Console](https://console.anthropic.com/)

### 2. å¦‚ä½•å¤„ç†é”™è¯¯?

```python
try:
    response = client.simple_chat(prompt="ä½ å¥½", model="gpt-3.5-turbo")
    print(response)
except requests.exceptions.HTTPError as e:
    print(f"HTTPé”™è¯¯: {e}")
except Exception as e:
    print(f"é”™è¯¯: {e}")
```

### 3. å¦‚ä½•ä½¿ç”¨ä»£ç†?

```python
# æ–¹æ³•1: é€šè¿‡è‡ªå®šä¹‰URL
client = create_client(
    platform="openai",
    api_key="sk-...",
    base_url="https://your-proxy.com/v1"
)

# æ–¹æ³•2: ä½¿ç”¨requestsçš„ä»£ç†
import os
os.environ['HTTP_PROXY'] = 'http://proxy.example.com:8080'
os.environ['HTTPS_PROXY'] = 'http://proxy.example.com:8080'
```

### 4. æ”¯æŒå¼‚æ­¥å—?

å½“å‰ç‰ˆæœ¬æ˜¯åŒæ­¥å®ç°ã€‚å¦‚éœ€å¼‚æ­¥æ”¯æŒï¼Œå¯ä»¥ä½¿ç”¨ `asyncio` + `aiohttp` æ”¹é€ å®¢æˆ·ç«¯ã€‚

## å¼€å‘

### é¡¹ç›®ç»“æ„

```
llm_sdk/
â”œâ”€â”€ __init__.py           # åŒ…å…¥å£
â”œâ”€â”€ base.py              # åŸºç¡€æ¥å£å®šä¹‰
â”œâ”€â”€ factory.py           # å®¢æˆ·ç«¯å·¥å‚
â”œâ”€â”€ ark_client.py        # ARKå®¢æˆ·ç«¯
â”œâ”€â”€ openai_client.py     # OpenAIå®¢æˆ·ç«¯
â””â”€â”€ anthropic_client.py  # Anthropicå®¢æˆ·ç«¯

llm_sdk_examples.py      # ä½¿ç”¨ç¤ºä¾‹
```

### æ·»åŠ æ–°å¹³å°

1. åœ¨ `llm_sdk/` ä¸‹åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯æ–‡ä»¶
2. ç»§æ‰¿ `BaseLLMClient` å¹¶å®ç°æ¥å£
3. åœ¨ `factory.py` ä¸­æ³¨å†Œæ–°å¹³å°

## License

MIT License

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼
